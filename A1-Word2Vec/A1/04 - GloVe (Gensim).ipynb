{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe (Gensim)\n",
    "\n",
    "To analyze word vectors, we will utilize Gensim. While not strictly a deep learning library, Gensim is a highly efficient and scalable tool for modeling text and word similarity. It originally focused on topic models like LDA but has since expanded to include SVD and various neural word representations. But its efficient and scalable, and quite widely used.   We gonna use **GloVe** embeddings, downloaded at [the Glove page](https://nlp.stanford.edu/projects/glove/). They're inside [this zip file](https://nlp.stanford.edu/data/glove.6B.zip)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader as api\n",
    "\n",
    "model = api.load('glove-wiki-gigaword-100')\n",
    "glove = api.load(\"glove-wiki-gigaword-100\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_analogy(a, b, c, model):\n",
    "    # Use gensim's built-in analogy via most_similar\n",
    "    if any(w not in model.key_to_index for w in (a, b, c)):\n",
    "        return None\n",
    "\n",
    "    for word, _ in model.most_similar(positive=[b, c], negative=[a], topn=10):\n",
    "        if word not in {a, b, c}:\n",
    "            return word\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_analogies(file_path, model):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith(\":\"):\n",
    "                continue\n",
    "\n",
    "            words = line.split()\n",
    "            if len(words) != 4:\n",
    "                continue\n",
    "\n",
    "            a, b, c, d = words\n",
    "\n",
    "            # Skip if any word is OOV\n",
    "            if any(w not in model.key_to_index for w in (a, b, c, d)):\n",
    "                continue\n",
    "\n",
    "            prediction = predict_analogy(a, b, c, model)\n",
    "            if prediction is None:\n",
    "                continue\n",
    "\n",
    "            total += 1\n",
    "            if prediction == d:\n",
    "                correct += 1\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "    return accuracy, correct, total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5544871794871795, 865, 1560)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_analogies(\"past-tense.txt\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_predict_capital_country(cap1, country1, cap2, glove): # Predict using glove\n",
    "    try:\n",
    "        return glove.most_similar(\n",
    "            positive=[country1, cap2],\n",
    "            negative=[cap1],\n",
    "            topn=1\n",
    "        )[0][0]\n",
    "    except KeyError:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_predict_capital_country(\"Athens\", \"Greece\", \"Berlin\", glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: GloVe Analysis using Gensim\n",
    "\n",
    "This notebook explores the use of pre-trained **GloVe (Global Vectors for Word Representation)** embeddings to perform semantic tasks and evaluate model performance using the **Gensim** library.\n",
    "\n",
    "### Key Tasks & Implementations:\n",
    "* **Pre-trained Model Loading**: \n",
    "    * Utilized the `gensim.downloader` API to load the `glove-wiki-gigaword-100` model.\n",
    "    * Leveraged efficient word vector storage via `KeyedVectors`.\n",
    "* **Analogy Prediction**: \n",
    "    * Implemented a `predict_analogy` function using the standard vector arithmetic formula: $Word_b - Word_a + Word_c \\approx Word_d$.\n",
    "    * Used Gensim's `most_similar` method to find the closest vector match for target analogies.\n",
    "* **Performance Evaluation**: \n",
    "    * Conducted systematic testing using the `past-tense.txt` analogy dataset.\n",
    "    * Calculated an accuracy score (e.g., ~55.45%) across valid test pairs to measure the model's grasp of grammatical transformations.\n",
    "* **Semantic Applications**: \n",
    "    * Developed specific utility functions, such as `glove_predict_capital_country`, to demonstrate the model's ability to map geographical and political relationships (e.g., Athens is to Greece as Berlin is to Germany).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
